# -*- coding: utf-8 -*-
"""Report for CTI Extracted Triples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18wD0WvKAeVsS5s2NhPnt9H5lFFfta4wf

# Cover Page

### **Title:** Performance of Large Language Models in Building Cybersecurity   Knowledge Graphs from Cyber Threat Intelligence Reports

### **Semester**: 2024 Fall Semester

### **Contacts**


*   Omokunle Oguntade; email: ooguntade@angelo.edu
*   Hoang Long Nguyen; email: hnguyen24@angelo.edu



-- Instructor: Dr. Erdogan Dogdu; email: erdogan.dogdu@angelo.edu

### **Project description**

In today's cybersecurity landscape, threats are evolving rapidly, and traditional methods of threat
detection and mitigation are no longer sufficient to handle the scale and complexity of modern cyber-
attacks. This project aims to harness the power of data-driven cybersecurity by utilizing large datasets
and advanced modeling techniques to create actionable threat intelligence. Specifically, explores the use
of knowledge graphs to improve the understanding and anticipation of cyber threats. Knowledge graphs
allow us to represent and analyze relationships between various elements, such as threat actors, attack
vectors, vulnerabilities, and organizations, offering a visual and predictive model for cyber defense.
In the rapidly evolving field of data-driven cybersecurity, the need for advanced, actionable intelligence is
paramount. The project focuses on the development and implementation of a sophisticated knowledge
graph framework to revolutionize the way threat intelligence is gathered, analyzed, and deployed. By
leveraging the interconnected nature of knowledge graphs, this project aims to provide deeper insights
into threat data, enabling cybersecurity professionals to predict, detect, and mitigate potential security
threats more effectively.
The project would utilize established/known research work in the extraction of triples from
existing Cyber Threat Intelligence (CTI) datasets using a similar prior work, titled “Actionable
Cyber Threat Intelligence using Knowledge Graphs and Large Language Models” as a guide to help use the Large Language Models (LLMs) for the extraction of triples from CTIs.

# Setup and Configs (credentials, etc.)

You will need to install one Llama model at a time if using T4 GPU to avoid out of GPU

### Requirement Library

Install requirement for Install Langchain and OpenAI
"""

!pip install langchain
!pip install langchain[all]
!pip install langchain-community
!pip install langchain langchain-community
!pip install --upgrade pip
!pip install --upgrade langchain
! pip install openai
! pip install openai==0.28
!pip install -U langchain-openai

"""Install requirement for Llama models"""

!pip install transformers datasets accelerate
!pip install -U bitsandbytes
!pip install -U accelerate
!apt-get install cuda-11-0
!pip install bitsandbytes

"""### Key API for OpenAI and Huggingface"""

# OpenAI API KEY
openai.api_key = 'need key'

# Loggins to Hugging Face
colab_token = "need key'"
from huggingface_hub import login
login(token="need key'")

"""### Install GPT 3.5 Turbo model"""

import re
import pandas as pd
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableSequence, RunnableLambda
from langchain_openai import ChatOpenAI  # Updated import path for ChatOpenAI

# Initialize the chat model
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.0,
    max_tokens=150,
    openai_api_key="need key'"
)

"""### Install GPT-4-mini model"""

import re
import pandas as pd
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableSequence, RunnableLambda
from langchain_openai import ChatOpenAI  # Updated import path for ChatOpenAI

# Initialize the chat model
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.0,
    max_tokens=150,
    openai_api_key="need key'"
)



"""### Install Llama 2-13-hf Model"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Model name
model_name_2 = "meta-llama/Llama-2-13b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model_name_2)
model = AutoModelForCausalLM.from_pretrained(model_name_2, device_map="auto", torch_dtype=torch.float16)

# Move to GPU for faster processing
model.to("cuda")
# Set the pad_token_id to avoid warnings
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

"""### Install Llama 3.1-8B Model"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Model name
model_name = "meta-llama/Llama-3.1-8B"

# Load tokenizer and model with 8-bit quantization and mixed precision



tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)

# Set the pad_token_id to avoid warnings
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

"""# Loading the Data Set"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
dir_dev = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/dev.csv'

dev_df = pd.read_csv(dir_dev, delimiter='\t')

# Check the first few rows to confirm successful loading
dev_df

dev_df.shape

dev_df.groupby('label').count()

import pandas as pd
dir_train = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/train.csv'

train_df = pd.read_csv(dir_train, delimiter='\t')

# Check the first few rows to confirm successful loading
train_df

# Save the DataFrame to a new file in your Google Drive
output_dir = '/content/drive/MyDrive/2024 FALL - Project (CS-6399-010)/Datasets CTI/dev_copy.csv'
dev_df.to_csv(output_dir, index=False)

text_dev = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/dev.txt'

# Import necessary libraries
import pandas as pd
import re

# Read the files
with open(text_dev, 'r') as file:
    dev_data = file.readlines()

# Function to clean and format the text
def clean_and_format_text(text_list):
    cleaned_text = []
    sentence = []

    for line in text_list:
        # Remove "ATK" and "O" markers
        line_cleaned = re.sub(r'\b(ATK|O)\b', '', line)

        # Remove any extra whitespaces
        line_cleaned = ' '.join(line_cleaned.split())

        # If the line is not empty after cleaning, continue processing
        if line_cleaned:
            sentence.append(line_cleaned)  # Build up the sentence line by line

            # If the line ends with a period, treat it as the end of a sentence
            if line_cleaned.endswith('.'):
                # Join the sentence list to form a full sentence, capitalize, and add to the cleaned text list
                full_sentence = ' '.join(sentence).strip().capitalize()
                cleaned_text.append(full_sentence)
                sentence = []  # Reset for the next sentence

    # Handle any remaining text that doesn't end with a period
    if sentence:
        full_sentence = ' '.join(sentence).strip().capitalize()
        cleaned_text.append(full_sentence)

    return cleaned_text

# Clean the datasets
cleaned_dev = clean_and_format_text(dev_data)

# Display the cleaned text data
cleaned_dev

# Join the cleaned list into a single paragraph
paragraph = ' '.join(cleaned_dev)

# Remove any unnecessary spaces before punctuation marks
paragraph = re.sub(r'\s+([.,;:?!])', r'\1', paragraph)

# Display the paragraph
print(paragraph)



"""# Set prompts

## Prompt for OpenAI 3.5-Turbo
"""

# Helper function to split large text into smaller chunks
def split_text_into_chunks(text, chunk_size=400):
    sentences = text.split('. ')
    chunks, chunk = [], ""

    for sentence in sentences:
        if len(chunk) + len(sentence) < chunk_size:
            chunk += sentence + ". "
        else:
            chunks.append(chunk.strip())
            chunk = sentence + ". "
    if chunk:
        chunks.append(chunk.strip())
    return chunks

# Define the prompt template for extracting triples
prompt_template = PromptTemplate(
    template="""
You are tasked with extracting subject-predicate-object triples from the following text.
Extract all the subject-predicate-object triples present, and return them in the format (subject, predicate, object).
Make sure to identify relationships based on cybersecurity concepts, such as malware, exploits, vulnerabilities, threat actors, operating systems, etc.

### Examples:
1. Text: "A new malware exploits a vulnerability in Windows systems to gain unauthorized access."
   Triple: (malware, exploits, vulnerability)
2. Text: "The threat actor used spyware to monitor users' activities on mobile devices."
   Triple: (threat actor, uses, spyware)
3. Text: "A phishing attack targets financial services to steal sensitive data."
   Triple: (phishing attack, targets, financial services)

Now, please extract triples from the following text:

Text: "{chunk}"
""",
    input_variables=["chunk"]
)

# Create a RunnableSequence that combines the prompt formatting and model
triple_extractor_chain = RunnableSequence(
    RunnableLambda(lambda inputs: {"chunk": inputs["chunk"]}),  # Prepare input for the prompt
    prompt_template,
    llm
)

# Main function to extract triples using LLM and handle smaller text chunks
def extract_triples_llm(text):
    chunks = split_text_into_chunks(text)
    all_triples = []

    for chunk in chunks:
        try:
            # Generate response for each chunk using LangChain
            response = triple_extractor_chain.invoke({"chunk": chunk})

            # Correctly access the response content
            response_content = response.content.strip()

            # Debugging step to see model output
            print(f"Response for chunk: {response_content}")

            # Process the response into triples
            if response_content:
                triples = re.findall(r"\((.*?), (.*?), (.*?)\)", response_content)
                processed_triples = [(subj.strip(), verb.strip(), obj.strip()) for subj, verb, obj in triples]

                # Add extracted triples to the final list along with the chunk they came from
                all_triples.extend([(chunk, subj, verb, obj) for subj, verb, obj in processed_triples])
            else:
                print(f"No response content for chunk: {chunk}")

        except Exception as e:
            print(f"Error processing text: {e}")

    return all_triples

# Function to process the DataFrame and extract triples
def turbo_process_dev_df(dev_df):
    all_extracted_triples = []  # To collect triples from all rows
    for idx, row in dev_df.iterrows():
        sentence = row['text']
        print(f"Processing Sentence {idx + 1}: {sentence}")
        extracted_triples = extract_triples_llm(sentence)
        print("Extracted Triples:")

        if extracted_triples:
            all_extracted_triples.extend([(sentence, triple[1], triple[2], triple[3]) for triple in extracted_triples])
            for triple in extracted_triples:
                print(triple)
        else:
            print("No relevant triples found.")
        print("-" * 50)

    return all_extracted_triples

"""## Prompt for Llama 2-13-hf"""

def generate_cybersecurity_triples(text):
    # Define a refined and focused prompt
    prompt = (
     "# Task:\n"
    "Extract concise and meaningful cybersecurity-related triples in the format (subject : predicate : object) from the provided text. "
    "Focus on capturing distinct relationships involving cybersecurity entities such as malware, exploits, devices, threat actors, and specific actions.\n\n"

    "# Examples:\n"
    "Text: \"A new malware exploits a vulnerability in Windows systems to gain unauthorized access.\"\n"
    "Extracted Triples: (malware : exploits : vulnerability); (malware : targets : Windows systems); (malware : gains : unauthorized access)\n\n"

    "Text: \"The implant’s functionality includes recording audio via the microphone, stealing messages via Accessibility Services, and connecting to controlled networks.\"\n"
    "Extracted Triples: (implant : includes : recording audio); (implant : steals : messages via Accessibility Services); (implant : connects : controlled networks)\n\n"

    "Text: \"A phishing attack targets financial institutions to steal sensitive information.\"\n"
    "Extracted Triples: (phishing attack : targets : financial institutions); (phishing attack : steals : sensitive information)\n\n"

    "# Now, extract only valid, meaningful triples directly from the following text. Discard any triple that includes words not present in the text:\n"
    f"Text: \"{text}\"\n\n"
    "Extracted Triples:"
)
    # Tokenize and generate output
    inputs = tokenizer(prompt, return_tensors="pt", padding=True)
    inputs = inputs.to("cuda")
    inputs["attention_mask"] = (inputs["input_ids"] != tokenizer.pad_token_id).to("cuda")

    # Generate output with deterministic settings
    outputs = model.generate(
        inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=250,  # Increased limit for longer responses
        do_sample=False,  # Deterministic generation
        temperature=0.0,  # Enforce the most likely output
        top_k=None  # Avoid setting sampling-related parameters
    )

    # Decode and process the output
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    extracted_triples = result.split("Extracted Triples:")[-1].strip()

    return extracted_triples

# Test sentence
example_sentence = (
    "Since then, the implant’s functionality has been improving and remarkable new features implemented, such as the ability to record audio surroundings via the microphone when an infected device is in a specified location; the stealing of WhatsApp messages via Accessibility Services; and the ability to connect an infected device to Wi-Fi networks controlled by cybercriminals."
)

# Generate triples for the sentence
extracted_triples = generate_cybersecurity_triples(example_sentence)
print("Extracted Triples:", extracted_triples)

"""## Prompt for Llama 3.1-8B"""

def generate_cybersecurity_triples(text):
    prompt = (
        "Identify and extract precise **subject-predicate-object triples** that reflect key cybersecurity relationships directly from the provided text. "
        "Each triple should convey a clear and complete relationship in the cybersecurity context (e.g., 'malware steals data' or 'attack targets system'). "
        "Follow these specific instructions for accurate triple extraction:\n\n"

        "### Instructions:\n"
        "1. **Exact Match**: Use only words directly from the text. Avoid adding any words or making inferences.\n"
        "2. **Complete Relationships**: Extract triples that represent full, meaningful relationships related to cybersecurity, such as actions taken by malware, impacts on systems, or data theft.\n"
        "3. **Avoid Redundancy**: Only include unique triples without repetitions or incomplete triples.\n\n"

        "### Examples:\n"
        "- Text: \"Malware exploits a vulnerability in Windows systems to gain unauthorized access.\"\n"
        "  - Extracted Triples: (malware : exploits : vulnerability); (malware : gains : unauthorized access)\n\n"

        "- Text: \"The ransomware encrypts files and demands payment for decryption.\"\n"
        "  - Extracted Triples: (ransomware : encrypts : files); (ransomware : demands : payment for decryption)\n\n"

        "- Text: \"A phishing attack targets bank customers, stealing credentials.\"\n"
        "  - Extracted Triples: (phishing attack : targets : bank customers); (phishing attack : steals : credentials)\n\n"

        "Now extract all relevant cybersecurity triples from the following text, using only exact terms from the input:\n"
        f"Text: \"{text}\"\n"
        "Extracted Triples:"
    )

    # Tokenize and generate output
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to("cuda")
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=100,  # Adjust as needed to limit length
        do_sample=False,
        temperature=0.0
    )

    # Decode and clean the output
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    extracted_triples = result.split("Extracted Triples:")[-1].strip().split(";")

    # Process to remove duplicates and return unique triples
    unique_triples = list(set(triple.strip() for triple in extracted_triples if triple.strip()))

    return unique_triples if unique_triples else ["No relevant triples."]

# Test sentence
example_sentence = (
    "Once FakeSpy is on the device, it steals all contacts in the device’s contact list and their information, as well as the infected device’s data."
)
extracted_triples = generate_cybersecurity_triples(example_sentence)
print("Extracted Triples:", extracted_triples)

"""## Prompt for GPT-4-mini"""

# Helper function to split large text into smaller chunks
def split_text_into_chunks(text, chunk_size=400):
    sentences = text.split('. ')
    chunks, chunk = [], ""

    for sentence in sentences:
        if len(chunk) + len(sentence) < chunk_size:
            chunk += sentence + ". "
        else:
            chunks.append(chunk.strip())
            chunk = sentence + ". "
    if chunk:
        chunks.append(chunk.strip())
    return chunks

# Define the prompt template for extracting triples
prompt_template = PromptTemplate(
    template="""
You are tasked with extracting subject-predicate-object triples from the following text.
Extract all the subject-predicate-object triples present, and return them in the format (subject, predicate, object).
Make sure to identify relationships based on cybersecurity concepts, such as malware, exploits, vulnerabilities, threat actors, operating systems, etc.

### Examples:
1. Text: "A new malware exploits a vulnerability in Windows systems to gain unauthorized access."
   Triple: (malware, exploits, vulnerability)
2. Text: "The threat actor used spyware to monitor users' activities on mobile devices."
   Triple: (threat actor, uses, spyware)
3. Text: "A phishing attack targets financial services to steal sensitive data."
   Triple: (phishing attack, targets, financial services)

Now, please extract triples from the following text:

Text: "{chunk}"
""",
    input_variables=["chunk"]
)

# Create a RunnableSequence that combines the prompt formatting and model
triple_extractor_chain = RunnableSequence(
    RunnableLambda(lambda inputs: {"chunk": inputs["chunk"]}),  # Prepare input for the prompt
    prompt_template,
    llm
)

# Main function to extract triples using LLM and handle smaller text chunks
def extract_triples_llm(text):
    chunks = split_text_into_chunks(text)
    all_triples = []

    for chunk in chunks:
        try:
            # Generate response for each chunk using LangChain
            response = triple_extractor_chain.invoke({"chunk": chunk})

            # Correctly access the response content
            response_content = response.content.strip()

            # Debugging step to see model output
            print(f"Response for chunk: {response_content}")

            # Process the response into triples
            if response_content:
                triples = re.findall(r"\((.*?), (.*?), (.*?)\)", response_content)
                processed_triples = [(subj.strip(), verb.strip(), obj.strip()) for subj, verb, obj in triples]

                # Add extracted triples to the final list along with the chunk they came from
                all_triples.extend([(chunk, subj, verb, obj) for subj, verb, obj in processed_triples])
            else:
                print(f"No response content for chunk: {chunk}")

        except Exception as e:
            print(f"Error processing text: {e}")

    return all_triples

# Function to process the DataFrame and extract triples
def process_dev_df(dev_df):
    all_extracted_triples = []  # To collect triples from all rows
    for idx, row in dev_df.iterrows():
        sentence = row['text']
        print(f"Processing Sentence {idx + 1}: {sentence}")
        extracted_triples = extract_triples_llm(sentence)
        print("Extracted Triples:")

        if extracted_triples:
            all_extracted_triples.extend([(sentence, triple[1], triple[2], triple[3]) for triple in extracted_triples])
            for triple in extracted_triples:
                print(triple)
        else:
            print("No relevant triples found.")
        print("-" * 50)

    return all_extracted_triples

"""# Extracted Triple for OpenAI 3.5-Turbo"""

all_triples = turbo_process_dev_df(dev_df)

# Create a DataFrame from the extracted triples
turbo_triples_df = pd.DataFrame(all_triples, columns=['Sentence', 'Subject', 'Predicate', 'Object'])

# Specify the path for saving the CSV
output_path = "/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/LangChain_GPT_extracted_triples.csv"

# Save the DataFrame to CSV
turbo_triples_df.to_csv(output_path, index=False)

print(f"Extracted triples have been saved to {output_path}")

"""##### Merging the triples into text and triples columns"""

import pandas as pd

# Create a DataFrame from the extracted triples
turbo_triples_df = pd.DataFrame(all_triples, columns=['Sentence', 'Subject', 'Predicate', 'Object'])

# Rename the 'Sentence' column to 'Text'
turbo_triples_df.rename(columns={'Sentence': 'Text'}, inplace=True)

# Format the 'Triples' column with colons and enclose the entire string in parentheses
turbo_triples_df['triples'] = '(' + turbo_triples_df['Subject'] + ' : ' + turbo_triples_df['Predicate'] + ' : ' + turbo_triples_df['Object'] + ')'

# Drop the original 'Subject', 'Predicate', and 'Object' columns as they are no longer needed
turbo_triples_df.drop(columns=['Subject', 'Predicate', 'Object'], inplace=True)

# Display the resulting DataFrame
print(turbo_triples_df.head())

# Specify the path for saving the CSV
output_path = "/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_Turbo_extracted_triples.csv"

# Save the DataFrame to CSV
turbo_triples_df.to_csv(output_path, index=False)



"""# Extracted Triple for GPT-4-mini"""

all_triples = process_dev_df(dev_df)

# Create a DataFrame from the extracted triples
triples_df = pd.DataFrame(all_triples, columns=['Sentence', 'Subject', 'Predicate', 'Object'])

# Specify the path for saving the CSV
output_path = "/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/LangChain_GPT_mini_extracted_triples.csv"

# Save the DataFrame to CSV
triples_df.to_csv(output_path, index=False)

print(f"Extracted triples have been saved to {output_path}")

"""##### Merging the triples into text and triples columns"""

import pandas as pd

# Create a DataFrame from the extracted triples
triples_df = pd.DataFrame(all_triples, columns=['Sentence', 'Subject', 'Predicate', 'Object'])

# Rename the 'Sentence' column to 'Text'
triples_df.rename(columns={'Sentence': 'Text'}, inplace=True)

# Format the 'Triples' column with colons and enclose the entire string in parentheses
triples_df['triples'] = '(' + triples_df['Subject'] + ' : ' + triples_df['Predicate'] + ' : ' + triples_df['Object'] + ')'

# Drop the original 'Subject', 'Predicate', and 'Object' columns as they are no longer needed
triples_df.drop(columns=['Subject', 'Predicate', 'Object'], inplace=True)

# Display the resulting DataFrame
print(triples_df.head())

# Specify the path for saving the CSV
output_path = "/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_mini_extracted_triples.csv"

# Save the DataFrame to CSV
triples_df.to_csv(output_path, index=False)



"""# Extracted Triple for Llama 2-13-hf"""

# Define the path in Google Drive where you want to save the file
output_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/llama2-13_extracted_triples.csv'

# Apply the triple extraction function to each entry in the 'sentence' column
dev_df['triples'] = dev_df['text'].apply(generate_cybersecurity_triples)
# Save the DataFrame to a CSV with 'sentence' and 'triples' columns in Google Drive
dev_df[['text', 'triples']].to_csv(output_file_path, index=False)
print(f"Extraction complete. Results saved to {output_file_path}")

import pandas as pd

# Load the file
file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/llama2-13_extracted_triples.csv'  # Replace with your file path
data = pd.read_csv(file_path)

# Define keywords to filter out rows that contain irrelevant comments, instructions, or notes
keywords = ["note", "task", "#", "example", "output", "expected", "hint", "constraints"]

# Filter out rows where any keyword is found in the 'text' column (case-insensitive)
cleaned_data = data[~data['text'].str.contains('|'.join(keywords), case=False, na=False)].copy()

# Remove any non-triple related text from the 'triples' column by ensuring it follows the format (subject : predicate : object)
# This regular expression filters rows that contain properly formatted triples
import re
pattern = r"\(\s*[^:]+?\s*:\s*[^:]+?\s*:\s*[^:]+?\s*\)"
cleaned_data['triples'] = cleaned_data['triples'].apply(lambda x: '; '.join(re.findall(pattern, x)) if pd.notna(x) else '')

# Strip any additional whitespace in the 'text' and 'triples' columns
cleaned_data['text'] = cleaned_data['text'].str.strip()
cleaned_data['triples'] = cleaned_data['triples'].str.strip()

# Remove rows where the 'triples' column is empty after cleaning
cleaned_data = cleaned_data[cleaned_data['triples'] != ""]

# Save the cleaned data to a new file
cleaned_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/final_cleaned_text_triples_Llama2.csv'  # This will save it in the current directory
cleaned_data.to_csv(cleaned_file_path, index=False)

print("File cleaned and saved as", cleaned_file_path)

import pandas as pd
triple_dev = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/final_cleaned_text_triples_Llama2.csv'

triple_df = pd.read_csv(triple_dev, delimiter='\t')

# Check the first few rows to confirm successful loading
triple_df

import re
import pandas as pd

# Function to separate text and triples based on first triple occurrence
def split_text_triples(row):
    # Use regex to find the first occurrence of a triple pattern
    match = re.search(r'\(\s*[^()]+?\s*:\s*[^()]+?\s*:\s*[^()]+?\s*\)', row)  # Pattern like (subject : predicate : object)

    if match:
        # Split at the beginning of the first matched triple
        split_index = match.start()
        text = row[:split_index].strip()
        triples = row[split_index:].strip()
    else:
        # If no match, assume the entire row is text
        text = row
        triples = ""

    return pd.Series([text, triples])

# Assuming 'triple_df' is the DataFrame with the combined 'text,triples' column
# Apply the function to separate the columns and assign column names
triple_df[['text', 'triples']] = triple_df['text,triples'].apply(split_text_triples)

# Drop the old combined column
df = triple_df[['text', 'triples']]

# Specify the output file path
output_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separate_cleaned_extracted_triples_Llama2-13.csv'

# Save the cleaned DataFrame to a CSV file, specifying no index and quoting to preserve formatting
df.to_csv(output_file_path, index=False, quoting=1)  # quoting=1 ensures all fields are quoted in the CSV file

print(f"File successfully saved to {output_file_path}")

"""Final file for extracted triples"""

import pandas as pd
dir_triple = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separate_cleaned_extracted_triples_Llama2-13.csv'

triple_df = pd.read_csv(dir_triple)

# Check the first few rows to confirm successful loading
triple_df

"""# Extracted Triple for Llama 3.1-8B"""

# Define the path in Google Drive where you want to save the file
output_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/llama3_extracted_triples.csv'

# Apply the triple extraction function to each entry in the 'sentence' column
dev_df['triples'] = dev_df['text'].apply(generate_cybersecurity_triples)

# Save the DataFrame to a CSV with 'sentence' and 'triples' columns in Google Drive
dev_df[['text', 'triples']].to_csv(output_file_path, index=False)
print(f"Extraction complete. Results saved to {output_file_path}")

import pandas as pd
import re

# Load the dataset
input_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/llama3_extracted_triples.csv'  # Update with your file path
output_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/reformatted_LLama3_Ltriples.csv'  # Update with your desired output path

# Read the CSV file
df = pd.read_csv(input_file_path)

# Function to clean and reformat triples
def clean_triples(triples_text):
    if pd.isna(triples_text):  # Check if the cell is empty or NaN
        return ""

    # Use regex to capture triples in the form (subject : predicate : object)
    triples = re.findall(r'\(([^:]+?)\s*:\s*([^:]+?)\s*:\s*([^)]+?)\)', triples_text)

    # Remove any extra whitespace and deduplicate triples
    cleaned_triples = {(subject.strip(), predicate.strip(), obj.strip()) for subject, predicate, obj in triples}

    # Format triples back as a single string for each row
    formatted_triples = '; '.join([f"({subject} : {predicate} : {obj})" for subject, predicate, obj in cleaned_triples])

    return formatted_triples

# Apply the function to clean the triples in each row
df['triples'] = df['triples'].apply(clean_triples)

# Save the reformatted DataFrame to a new CSV file
df.to_csv(output_file_path, index=False)

print(f"Reformatted triples saved to {output_file_path}")

import pandas as pd
triple_dev = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/reformatted_LLama3_Ltriples.csv'

triple_df = pd.read_csv(triple_dev, delimiter='\t')

# Check the first few rows to confirm successful loading
triple_df

import re
import pandas as pd

# Function to separate text and triples based on first triple occurrence
def split_text_triples(row):
    # Use regex to find the first occurrence of a triple pattern
    match = re.search(r'\(\s*[^()]+?\s*:\s*[^()]+?\s*:\s*[^()]+?\s*\)', row)  # Pattern like (subject : predicate : object)

    if match:
        # Split at the beginning of the first matched triple
        split_index = match.start()
        text = row[:split_index].strip()
        triples = row[split_index:].strip()
    else:
        # If no match, assume the entire row is text
        text = row
        triples = ""

    return pd.Series([text, triples])

# Assuming 'triple_df' is the DataFrame with the combined 'text,triples' column
# Apply the function to separate the columns and assign column names
triple_df[['text', 'triples']] = triple_df['text,triples'].apply(split_text_triples)

# Drop the old combined column
df = triple_df[['text', 'triples']]

# Specify the output file path
output_file_path = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separated_text_LLama3_triples.csv'

# Save the cleaned DataFrame to a CSV file, specifying no index and quoting to preserve formatting
df.to_csv(output_file_path, index=False, quoting=1)  # quoting=1 ensures all fields are quoted in the CSV file

print(f"File successfully saved to {output_file_path}")

"""Final file for extracted triples"""

import pandas as pd
dir_triple = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separated_text_LLama3_triples.csv'

triple_df = pd.read_csv(dir_triple)

# Check the first few rows to confirm successful loading
triple_df

"""# Report Performance by Cross-validation"""

!pip install pandas nltk

!pip install rouge-score

"""###  Rouge Number
--- **file names:**


*   separate_cleaned_extracted_triples_Llama2-13.csv, separated_text_LLama3_triples.csv,
*   seperated_LangChain_GPT_Turbo_extracted_triples.csv,
seperated_LangChain_GPT_mini_extracted_triples.csv

##### Merege Triples with File has split triples
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_mini_extracted_triples.csv')

# Add a unique identifier for each row to preserve the original order
df["order"] = df.index

# Group by the 'text' column and merge all triples into one string, keeping the order intact
merged_df = df.groupby("Text").agg({
    "triples": lambda x: "; ".join(x),  # Merge triples
    "order": "min"  # Retain the original order of the first occurrence
}).reset_index()

# Sort the grouped dataframe by the preserved order
merged_df = merged_df.sort_values("order").drop(columns=["order"])

# Reset the index so it starts from 0
merged_df = merged_df.reset_index(drop=True)

# Rename the columns for clarity
merged_df.columns = ["text", "triples"]

# Save the result to a new CSV file
merged_df.to_csv('/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/grouped_sentences_and_triples_LangChain_GPT_mini_extracted_triples.csv', index=False)

print("The file has been processed and saved as 'grouped_sentences_and_triples_LangChain_GPT_mini_extracted_triples.csv'")

import pandas as pd

# Load the dataset
input_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_Turbo_extracted_triples.csv'

df = pd.read_csv('/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_Turbo_extracted_triples.csv')

# Add a unique identifier for each row to preserve the original order
df["order"] = df.index

# Group by the 'text' column and merge all triples into one string, keeping the order intact
merged_df = df.groupby("Text").agg({
    "triples": lambda x: "; ".join(x),  # Merge triples
    "order": "min"  # Retain the original order of the first occurrence
}).reset_index()

# Sort the grouped dataframe by the preserved order
merged_df = merged_df.sort_values("order").drop(columns=["order"])

# Reset the index so it starts from 0
merged_df = merged_df.reset_index(drop=True)

# Rename the columns for clarity
merged_df.columns = ["text", "triples"]

# Save the result to a new CSV file
merged_df.to_csv('/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/grouped_sentences_and_triples_LangChain_GPT_Turbo_extracted_triples.csv', index=False)

print("The file has been processed and saved as 'grouped_sentences_and_triples_LangChain_GPT_Turbo_extracted_triples.csv'")

"""##### ROUGE Number using OPENAI Turbo-3.5 as reference"""

import re
import pandas as pd
from collections import Counter
from itertools import chain
from nltk.util import ngrams
from typing import List, Dict
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

def load_triples_from_csv(filename: str) -> List[str]:
    """Load triples from a CSV file, assuming each triple is in a column named 'triples'."""
    df = pd.read_csv(filename)
    return df['triples'].dropna().tolist()  # Drop any NaN values and convert to list

def get_ngrams(triples: List[str], n: int) -> Counter:
    """Convert triples into n-grams and count their occurrences."""
    all_ngrams = chain(*[ngrams(re.findall(r'\w+', triple.lower()), n) for triple in triples])
    return Counter(all_ngrams)

def lcs(X: List[str], Y: List[str]) -> int:
    """Calculate the length of the longest common subsequence between two sequences."""
    m, n = len(X), len(Y)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(m):
        for j in range(n):
            if X[i] == Y[j]:
                dp[i + 1][j + 1] = dp[i][j] + 1
            else:
                dp[i + 1][j + 1] = max(dp[i + 1][j], dp[i][j + 1])
    return dp[m][n]

def calculate_rough_rouge(reference_file: str, comparison_file: str) -> Dict[str, float]:
    """Calculate rough ROUGE scores for n-grams and ROUGE-L for a single pair of files."""
    reference_triples = load_triples_from_csv(reference_file)
    comparison_triples = load_triples_from_csv(comparison_file)

    rouge_scores = {}

    # Calculate ROUGE-n scores (1, 2, 3, and 6)
    for n in [1, 2, 3, 6]:
        ref_ngrams = get_ngrams(reference_triples, n)
        comp_ngrams = get_ngrams(comparison_triples, n)

        overlap = sum((ref_ngrams & comp_ngrams).values())
        total_ref = sum(ref_ngrams.values())
        rouge_scores[f"ROUGE-{n}"] = overlap / total_ref if total_ref > 0 else 0

    # Calculate ROUGE-L by averaging the LCS scores over all pairs
    lcs_scores = [
        lcs(ref.split(), comp.split()) / max(len(ref.split()), len(comp.split()))
        for ref in reference_triples for comp in comparison_triples
    ]
    rouge_scores["ROUGE-L"] = sum(lcs_scores) / len(lcs_scores) if lcs_scores else 0

    return rouge_scores

def compare_models(reference_file: str, comparison_files: Dict[str, str]):
    """Compare multiple models against a single reference and print ROUGE scores."""
    for model_name, comp_file in comparison_files.items():
        rouge_scores = calculate_rough_rouge(reference_file, comp_file)
        print(f"\nROUGE scores for {model_name} compared to reference:")
        for rouge_type, score in rouge_scores.items():
            print(f"{rouge_type}: {score:.4f}")

# Example of usage:
# Update these paths according to your Google Drive file structure
reference_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_Turbo_extracted_triples.csv'
comparison_files = {
    "Llama 2-13-hf": '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separate_cleaned_extracted_triples_Llama2-13.csv',
    "Llama 3.1": '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separated_text_LLama3_triples.csv',
    "GPT 4o-mimi": '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/seperated_LangChain_GPT_mini_extracted_triples.csv'
}

# Run the comparison
compare_models(reference_file, comparison_files)



"""#### Calculate Rough Number"""

import re
import pandas as pd
from collections import Counter
from itertools import chain
from nltk.util import ngrams
from typing import List

def load_triples_from_csv(filename: str) -> List[str]:
    """Load triples from a CSV file, assuming each triple is in a column named 'triples'."""
    df = pd.read_csv(filename)
    return df['triples'].dropna().tolist()  # Drop any NaN values and convert to list

def get_ngrams(triples: List[str], n: int) -> Counter:
    """Convert triples into n-grams and count their occurrences."""
    all_ngrams = chain(*[ngrams(re.findall(r'\w+', triple.lower()), n) for triple in triples])
    return Counter(all_ngrams)

def calculate_rough_rouge(reference_file: str, comparison_file: str):
    """Calculate rough ROUGE scores for different n-grams without precision and recall."""
    reference_triples = load_triples_from_csv(reference_file)
    comparison_triples = load_triples_from_csv(comparison_file)

    rouge_scores = {}

    for n in [1, 2, 3, 6]:  # ROUGE-1, ROUGE-2, ROUGE-3, and ROUGE-6
        ref_ngrams = get_ngrams(reference_triples, n)
        comp_ngrams = get_ngrams(comparison_triples, n)

        overlap = sum((ref_ngrams & comp_ngrams).values())
        total_ref = sum(ref_ngrams.values())

        # Calculate rough score as the overlap normalized by reference size
        rouge_scores[f"ROUGE-{n}"] = overlap / total_ref if total_ref > 0 else 0

    # ROUGE-L Calculation: based on the longest common subsequence
    def lcs(X, Y):
        m, n = len(X), len(Y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(m):
            for j in range(n):
                if X[i] == Y[j]:
                    dp[i + 1][j + 1] = dp[i][j] + 1
                else:
                    dp[i + 1][j + 1] = max(dp[i + 1][j], dp[i][j + 1])
        return dp[m][n]

    # Calculating ROUGE-L by averaging LCS scores normalized by max length of each reference-candidate pair
    lcs_scores = [
        lcs(ref.split(), comp.split()) / max(len(ref.split()), len(comp.split()))
        for ref in reference_triples for comp in comparison_triples
    ]
    rouge_scores["ROUGE-L"] = sum(lcs_scores) / len(lcs_scores) if lcs_scores else 0

    return rouge_scores

"""Get input file"""

GPT_Turbo_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/grouped_sentences_and_triples_LangChain_GPT_Turbo_extracted_triples.csv'
Llama_2_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separate_cleaned_extracted_triples_Llama2-13.csv'
Llama3_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/separated_text_LLama3_triples.csv'
GPT_mini_file = '/content/drive/MyDrive/CTI Triple Extraction Using LLM/2024 FALL - Project (CS-6399-010)/Datasets CTI/grouped_sentences_and_triples_LangChain_GPT_mini_extracted_triples.csv'

"""##### Mark GPT 3.5 Turbo as a reference"""

reference_file = GPT_Turbo_file
comparison_files = {
    "Llama 2-13-hf": Llama_2_file,
    "Llama 3.1-8B": Llama3_file,
    "GPT-Mini": GPT_mini_file
}


# Store results in a list of dictionaries for tabulation
results = []

for model, comp_file in comparison_files.items():
    rouge_scores = calculate_rough_rouge(reference_file, comp_file)
    results.append({
        "Model": model,
        "ROUGE-1": rouge_scores["ROUGE-1"],
        "ROUGE-2": rouge_scores["ROUGE-2"],
        "ROUGE-3": rouge_scores["ROUGE-3"],
        "ROUGE-6": rouge_scores["ROUGE-6"],
        "ROUGE-L": rouge_scores["ROUGE-L"]
    })

# Create a pandas DataFrame for the results
df_GPT_Turbo = pd.DataFrame(results)

# Function to highlight the maximum value in each column
def highlight_max(s):
    is_max = s == s.max()
    return ['font-weight: bold' if v else '' for v in is_max]

# Apply the highlight styling
styled_df_GTP_Turbo = df_GPT_Turbo.style.apply(highlight_max, subset=['ROUGE-1', 'ROUGE-2', 'ROUGE-3', 'ROUGE-6', 'ROUGE-L'], axis=0)

df_GPT_Turbo

"""##### Mark Llama 2-13 as the reference"""

reference_file = Llama_2_file
comparison_files = {
    "GPT-3.5 Turbo": GPT_Turbo_file,
    "Llama 3.1-8B": Llama3_file,
    "GPT-Mini": GPT_mini_file
}

# Store results in a list of dictionaries for tabulation
results = []

for model, comp_file in comparison_files.items():
    rouge_scores = calculate_rough_rouge(reference_file, comp_file)
    results.append({
        "Model": model,
        "ROUGE-1": rouge_scores["ROUGE-1"],
        "ROUGE-2": rouge_scores["ROUGE-2"],
        "ROUGE-3": rouge_scores["ROUGE-3"],
        "ROUGE-6": rouge_scores["ROUGE-6"],
        "ROUGE-L": rouge_scores["ROUGE-L"]
    })

# Create a pandas DataFrame for the results
df_Llama2 = pd.DataFrame(results)


# Apply the highlight styling
styled_df_Llama2 = df_Llama2.style.apply(highlight_max, subset=['ROUGE-1', 'ROUGE-2', 'ROUGE-3', 'ROUGE-6', 'ROUGE-L'], axis=0)

df_Llama2

"""##### Mark Llama 3.1-8B as reference"""

reference_file = Llama3_file
comparison_files = {
    "GPT-3.5 Turbo": GPT_Turbo_file,
    "Llama 2-13-hf": Llama_2_file,
    "GPT-Mini": GPT_mini_file
}

# Store results in a list of dictionaries for tabulation
results = []

for model, comp_file in comparison_files.items():
    rouge_scores = calculate_rough_rouge(reference_file, comp_file)
    results.append({
        "Model": model,
        "ROUGE-1": rouge_scores["ROUGE-1"],
        "ROUGE-2": rouge_scores["ROUGE-2"],
        "ROUGE-3": rouge_scores["ROUGE-3"],
        "ROUGE-6": rouge_scores["ROUGE-6"],
        "ROUGE-L": rouge_scores["ROUGE-L"]
    })

# Create a pandas DataFrame for the results
df_Llama3 = pd.DataFrame(results)

df_Llama3

"""##### Makr GPT mini as reference"""

reference_file = GPT_mini_file
comparison_files = {
    "GPT-3.5 Turbo": GPT_Turbo_file,
    "Llama 3.1-8B": Llama3_file,
    "LLama 2-13-hf": Llama_2_file
}

# Store results in a list of dictionaries for tabulation
results = []

for model, comp_file in comparison_files.items():
    rouge_scores = calculate_rough_rouge(reference_file, comp_file)
    results.append({
        "Model": model,
        "ROUGE-1": rouge_scores["ROUGE-1"],
        "ROUGE-2": rouge_scores["ROUGE-2"],
        "ROUGE-3": rouge_scores["ROUGE-3"],
        "ROUGE-6": rouge_scores["ROUGE-6"],
        "ROUGE-L": rouge_scores["ROUGE-L"]
    })


# Create a pandas DataFrame for the results
df_GPT_mini = pd.DataFrame(results)

df_GPT_mini

"""# Compare results

## Cross Checking 4 Rough number table
"""

# Highlight the best scores for each metric
def highlight_best(df, metric_cols):
    """Highlight the best score in each row for the given metric columns."""
    def highlight(s):
        is_max = s == s.max()
        return ['font-weight: bold' if v else '' for v in is_max]

    return df.style.apply(highlight, subset=metric_cols, axis=1)

table_gpt_turbo = df_GPT_Turbo
table_llama2 = df_Llama2
table_llama3 = df_Llama3
table_gpt_mini = df_GPT_mini

# Combine the tables into one DataFrame
tables = {
    "GPT-3.5 Turbo": table_gpt_turbo,
    "Llama 2-13-hf": table_llama2,
    "Llama 3.1-8B": table_llama3,
    "GPT Mini": table_gpt_mini
}

# Add a column for reference and concatenate
consolidated_table = pd.concat(
    [table.assign(Reference=ref) for ref, table in tables.items()], ignore_index=True
)


# Metrics to compare
metric_cols = ["ROUGE-1", "ROUGE-2", "ROUGE-3", "ROUGE-6", "ROUGE-L"]

# Display the styled DataFrame
highlight_consolidated_table = highlight_best(consolidated_table, metric_cols)

# Display the table with highlights
highlight_consolidated_table

"""**Explaination about Result**

The ROUGE scores indicate the relative performance of each model in extracting subject-predicate-object triples when compared to a reference. Let me summarize and analyze the performance based on the uploaded results:

 1.  **GPT-3.5 Turbo as Reference**:
*   **Llama 2-13B-hf**: Achieved moderately high ROUGE-1 (0.5502) and slightly better ROUGE-L (0.1785) than Llama 3.1. This suggests it captured some key relationships accurately but struggled with finer-grained details (as seen in lower ROUGE-3 and ROUGE-6).
*   **Llama 3.1-8B:** Slightly lower ROUGE-1 (0.4732) and weaker performance on ROUGE-2, ROUGE-3, and ROUGE-6 compared to Llama 2-13B-hf, showing difficulty in capturing more complex or nuanced relationships.
*  **GPT-Mini**: Perfect scores for ROUGE-1 through ROUGE-6 (all 1.0000), which might indicate that GPT-Mini consistently reproduced the triples exactly as GPT-3.5 Turbo did.

 2. **Llama 2-13B-hf as Reference**:
*  **GPT-3.5 Turbo**: High ROUGE-1 (0.8495) and better scores across all metrics compared to Llama 3.1 and GPT-Mini. This suggests GPT-3.5 Turbo extracted relationships with a high degree of overlap with the Llama 2-13B-hf reference but included more nuanced variations (lower ROUGE-6).
*  **Llama 3.1-8B**: Lower ROUGE scores across all metrics compared to GPT-3.5 Turbo, indicating less overlap in triples. However, it still maintained reasonable performance on ROUGE-1 (0.6184).
*  **GPT-Mini**: Surprisingly identical scores to GPT-3.5 Turbo (e.g., ROUGE-1 = 0.8495), potentially indicating redundancy or reliance on similar triples between the two.

 3. **Llama 3.1-8B as Reference:**
*  **GPT-3.5 Turbo**: Scored highest across most metrics when compared to Llama 3.1, suggesting better overlap and higher-quality triples than other models when referenced against Llama 3.1.
*  **Llama 2-13B-hf**: Scored lower than GPT-3.5 Turbo but outperformed GPT-Mini on ROUGE-1 and ROUGE-L, indicating better alignment with the nuanced relationships extracted by Llama 3.1.
*  **GPT-Mini**: Lower ROUGE scores than GPT-3.5 Turbo and Llama 2-13B-hf, showing weaker overlap in triples extracted compared to Llama 3.1.

 4.  **GPT-Mini as Reference**
*  **GPT-3.5 Turbo**: Perfect scores (1.0000) for ROUGE-1 to ROUGE-6, indicating exact reproduction of triples when GPT-Mini is the reference. This suggests significant redundancy or exact copying of triples between the two.
*  **Llama 3.1-8B**: Lower scores across all metrics (e.g., ROUGE-1 = 0.4732), showing that it deviated substantially from GPT-Mini's extracted triples.
*  **Llama 2-13B-hf**: Performed slightly better than Llama 3.1 across ROUGE-1 to ROUGE-6, showing a closer match to GPT-Mini.
---


> Based on the evaluation, GPT-3.5 Turbo proved to be the best model for extracting subject-predicate-object triples in Cyber Threat Intelligence (CTI) tasks, achieving the highest ROUGE scores with consistent precision and contextual understanding. It effectively handled diverse sentence structures and nuanced relationships, making it highly reliable for CTI applications. Llama 2-13B-hf performed moderately well on simpler tasks but struggled with complex relationships, while Llama 3.1 showed weaker overall performance, particularly with ambiguous sentences. GPT-Mini, despite perfect ROUGE scores against GPT-3.5 Turbo, relied excessively on the reference, producing repetitive triples and lacking adaptability. Overall, GPT-3.5 Turbo is the most suitable model for CTI triple extraction.

## Confusion Matrix (More evaluation still need to improve)

###### Added reference_file to the table
"""

import pandas as pd

df_1 = df_GPT_Turbo

df_2 = df_Llama2

df_3 = df_Llama3

df_4 = df_GPT_mini



# Combine all tables
combined_reference_df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=False)
combined_reference_df
# print(combined_reference_df)

"""### Confusion Tables for Rouge Scores

Average Rouge Scores tables
"""

# Create a DataFrame
df = pd.DataFrame(combined_reference_df)

# Ensure model names are consistent (strip whitespace and correct capitalization)
df["Model"] = df["Model"].str.strip().str.title()

# Group by model and calculate the average ROUGE scores
average_scores = df.groupby("Model", as_index=False).mean()

# Display the result
print("Average ROUGE Scores for Each Model:")
average_scores

"""##### Create Confusion tables"""

# Function to calculate the confusion matrix for a specific ROUGE metric
def calculate_confusion_matrix(metric, scores_df):
    models = scores_df["Model"]
    values = scores_df[metric].values
    confusion_matrix = pd.DataFrame(
        [[round(abs(values[i] - values[j]), 6) if i != j else None for j in range(len(models))] for i in range(len(models))],
        index=models,
        columns=models
    )
    return confusion_matrix

# Create a DataFrame
df = pd.DataFrame(average_scores)
# Calculate confusion matrices for each ROUGE metric
metrics = ["ROUGE-1", "ROUGE-2", "ROUGE-3", "ROUGE-6", "ROUGE-L"]
confusion_matrices = {metric: calculate_confusion_matrix(metric, average_scores) for metric in metrics}

# Function to highlight the best score in each row
def highlight_best(s):
    is_best = s == s.max(skipna=True)  # Skip NaN values
    return ["font-weight: bold;" if v else "" for v in is_best]

# Display confusion matrices with highlighting
for metric, matrix in confusion_matrices.items():
    print(f"Confusion Matrix for {metric}:\n")
    styled_matrix = matrix.style.apply(highlight_best, axis=1)
    display(styled_matrix)

"""The confusion matrices provide a comparative analysis of performance for four LLMs—GPT-3.5 Turbo, GPT Mini, Llama 2-13-hf, and Llama 3.1—across five ROUGE metrics: ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-6, and ROUGE-L. Each matrix uses a specific LLM as a reference, and the values indicate the relative similarity scores of other models against this reference:


1.   **Rouge 1 Comparision:**


*   GPT Mini consistently shows dominance across all reference models, demonstrating superior performance in capturing unigram overlaps, which highlights its strong precision and recall for basic token similarities.
*  GPT-3.5 Turbo emerges as the second-best performer, with scores closely trailing GPT Mini. This indicates its robust performance in unigram precision, though slightly behind the leader.
*   Llama models exhibit noticeable gaps compared to the GPT models, showing weaker unigram overlaps, which suggests lower performance in basic token matching.

2.   **ROUGE-2 Comparisons:**


*   The trend remains similar to ROUGE-1, with GPT Mini leading the scores and GPT-3.5 Turbo maintaining a competitive position.
*   Llama 2-13-hf and Llama 3.1-8B continue to show reduced performance, indicating challenges in capturing bigram-level relationships effectively. This highlights the limitations of Llama models in understanding slightly more complex token sequences.


3.  **ROUGE-3 and ROUGE-6 Comparisons:**


*   These higher-order n-gram metrics reflect the models' ability to capture more complex overlaps involving trigrams and beyond.
*   GPT Mini retains its position as the top performer, showing its capability to extract detailed and meaningful relationships.
*   The gap between GPT models and Llama models becomes more pronounced, especially for ROUGE-6, where the Llama models show significant drops in scores. This highlights the challenges faced by Llama models in capturing intricate and contextually deeper relationships.


4.   **ROUGE-L Comparisons:**


*   This metric evaluates the longest common subsequence, emphasizing sentence structure.
*   GPT Mini continues to perform strongly, reflecting its ability to align with sentence structures effectively.
*   GPT-3.5 Turbo occasionally achieves comparable scores, showcasing its ability to handle sentence-level relationships, although it remains slightly behind GPT Mini.
*  Llama-based models show less competitive performance, struggling to match the sentence alignment capabilities of the GPT models.

**Conclusion**: The confusion matrices reveal that GPT Mini consistently outperforms other models across all ROUGE metrics, followed by GPT-3.5 Turbo. Llama 2-13-hf and Llama 3.1 show reasonable performance but are generally less competitive in this specific task. These results can guide the selection of LLMs for tasks requiring high textual similarity, favoring GPT Mini for optimal performance.








"""